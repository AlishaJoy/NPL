{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+SCCFr4dg6j0HqA9vuuBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlishaJoy/NPL/blob/main/NLP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP**"
      ],
      "metadata": {
        "id": "xqzd6K7fmi-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Natural Language Processing\n",
        " ---\n",
        "- Text classification\n",
        "---\n",
        "- Name entity Recognition: categorize names\n",
        "---\n",
        "- Text Summarisation : summary for long texts\n",
        "---\n",
        "- Question answering\n",
        "---\n",
        "- Machine Translation\n",
        "---\n",
        "- Sentiment analysis\n"
      ],
      "metadata": {
        "id": "w0WCipc3mi9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install NLP:"
      ],
      "metadata": {
        "id": "6ps95UzPsSaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48kF6LZpmR2q",
        "outputId": "c53412c5-250d-4e6d-cedd-c69252200cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization:\n",
        "*breaking down a sentence into multiple words*"
      ],
      "metadata": {
        "id": "GSkO92yesj15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def tokenize_text(text):\n",
        "  return nltk.word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtmRhjIpsaZq",
        "outputId": "fd8f9696-8bce-4f2e-fd85-8a13aab1ff95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize_text(\"welcome to todays session on NLP\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmoJTeTvtUg9",
        "outputId": "96ef9377-9cbc-48ae-856e-f0a531f1018d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome', 'to', 'todays', 'session', 'on', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop word removal:\n",
        "- it removes the stopwords like is,the,on etc"
      ],
      "metadata": {
        "id": "CPQtrWAAvCqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy5zeEEe9rQZ",
        "outputId": "5d381ed5-cce4-4761-d732-3cefcd0d8ccc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_stopwords(tokens):\n",
        "  # from nltk.corpus import stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  return[token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "text =\"welcome to nlp class of day one\"\n",
        "tokens = tokenize_text(text)\n",
        "print(tokens)\n",
        "print(remove_stopwords(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH35Ief-wxji",
        "outputId": "013cde27-50b1-4175-dd53-c8b6fef73ed6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome', 'to', 'nlp', 'class', 'of', 'day', 'one']\n",
            "['welcome', 'nlp', 'class', 'day', 'one']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #l =[]\n",
        "  #for token in tokens:\n",
        "   # if token.lower() not in stop_words:\n",
        "    #  l.append(token)\n",
        "#l"
      ],
      "metadata": {
        "id": "G3qKn2lBt1Zh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0fXlRHUyvviD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}